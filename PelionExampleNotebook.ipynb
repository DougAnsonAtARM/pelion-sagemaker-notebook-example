{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate a simple compile and deployment workflow using a ResNet50 pre-trained image recognizer with sample input images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define all of our parameters that will be used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup our Sagemaker configuration tunables\n",
    "aws_region = 'us-east-1'\n",
    "aws_s3_folder = \"DEMO-Sagemaker-Edge\"\n",
    "\n",
    "# We will use the ResNet50 image recognition model via Keras framework\n",
    "model_framework = 'keras'\n",
    "model_name = 'demo-' + model_framework\n",
    "model_basename = 'resnet50'\n",
    "image_size = 224\n",
    "image_list = []\n",
    "packaged_model_name = model_framework + \"-model\"\n",
    "packaged_model_version = \"1.0\"\n",
    "\n",
    "# Input images directory in our notebook...\n",
    "image_paths = './images'\n",
    "\n",
    "# Our Pelion Edge Gateway is a Nvidia Jetson Xavier\n",
    "target_device = 'jetson_xavier'\n",
    "\n",
    "# Set our Pelion API Configuration Here\n",
    "api_key = 'INSERT_YOUR_PELION_APPLICATION_KEY_HERE'\n",
    "device_id = 'INSERT_YOUR_PELION_XAVIER_EDGE_GATEWAY_DEVICEID_HERE' # Pelion Device ID of our Sagemaker Edge Agent PT device under our Pelion Edge gateway\n",
    "endpoint_api = 'api.' + aws_region + '.mbedcloud.com'              # This is optional and the default        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import some additional packages into our python environment... including the pelion/sagemaker controller package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets install some image utilities\n",
    "! pip install ipympl\n",
    "\n",
    "# Core imports for the notebook\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# We import these to help us specifically with our selected pre-trained model\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# We'll also use time for waiting on predictions to complete...\n",
    "import time\n",
    "\n",
    "# we will also use some helpers from numpy...\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "\n",
    "# We also need to install the Pelion Sagemaker Controller API\n",
    "! pip install pelion_sagemaker_controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the core class for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import example_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we allocate our notebook class... this will init both Sagamaker as well as the Pelion Controller API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_notebook = example_notebook.MyNotebook(api_key, device_id, endpoint_api, aws_s3_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets import a pre-trained Mobilenet v2 model via TF Hub..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model - we will choose the ResNet50 model pre-trained with imagenet weights...\n",
    "resnet50_model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Lets dump the model details... \n",
    "print(resnet50_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we prepare our input images and upload them to S3 as preprocessed images for ResNet50 from Keras..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect and prepare our images to analyze\n",
    "print(\"\")\n",
    "print(\"Collecting images from: \" + str(image_paths))\n",
    "my_image_list = [os.path.join(image_paths,filename) for filename in os.listdir(image_paths) if os.path.isfile(image_paths + '/' + filename)]\n",
    "print(\"\")\n",
    "print(\"Input Image Files: \" + json.dumps(my_image_list))\n",
    "\n",
    "# preprocess_input() is ResNet50 specific\n",
    "print(\"\")\n",
    "print(\"Pre-processing input image files...\")\n",
    "preprocessed_images_list = preprocess_input(my_notebook.read_and_prep_images(my_image_list, image_size, image_size))\n",
    "\n",
    "# Display our initial images\n",
    "print(\"Displaying Input Images:\")\n",
    "my_notebook.display_images(image_list)\n",
    "\n",
    "# Neo Sagemaker likes NCHW. Our image set is in NHCW... so we must convert it...\n",
    "print(\"\")\n",
    "print(\"Current input shape as NHCW: \" + str(preprocessed_images_list.shape))\n",
    "nchw_preprocessed_images_list = tf.transpose(preprocessed_images_list, [0, 3, 1, 2])\n",
    "print(\"\")\n",
    "print(\"Transposed input shape to NCHW: \" + str(nchw_preprocessed_images_list.shape))\n",
    "\n",
    "# Save the preprocessed images_list as a single input file locally... \n",
    "input_data_filename = 'preprocessed_images_' + str(time.time()) + '.input'\n",
    "input_data_filename_saved = input_data_filename + '.npy'\n",
    "\n",
    "# save to a numpy-compatible file\n",
    "save(input_data_filename, nchw_preprocessed_images_list)\n",
    "\n",
    "# Upload the images in the list to S3\n",
    "print(\"\")\n",
    "print('Uploading preprocessed input images to ' + my_notebook.iot_folder + \" in S3 bucket \" + my_notebook.bucket + ' as: ' + input_data_filename_saved + \"...\")\n",
    "print(\"\")\n",
    "my_notebook.sess.upload_data(input_data_filename_saved, my_notebook.bucket, my_notebook.iot_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compile up our model, then package it on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets configure our shape with the number of images we intend on analyzing (Keras-specific: Resnet50 is pulled from Keras framework)\n",
    "input_data_shape = '{\"input_1\":' + str(nchw_preprocessed_images_list.shape).replace('(','[').replace(')',']') + '}'\n",
    "print(\"\")\n",
    "print(\"Input Data Shape as NCHW (Required by Sagemaker Neo for Keras/ResNet50): \" + json.dumps(input_data_shape))\n",
    "\n",
    "# Compile up for our target Pelion Edge Gateway platform type\n",
    "print(\"\")\n",
    "print(\"Initiating Sagemaker Neo compile of \" + model_name + \"...\")\n",
    "job_name = my_notebook.compile_model(resnet50_model, target_device, model_basename, input_data_shape, model_framework)\n",
    "\n",
    "# Package up and store the compiled model onto S3\n",
    "print(\"\")\n",
    "print(\"Packaging up Neo-compiled model and placing in S3...\")\n",
    "model_package = my_notebook.package_model(packaged_model_name, packaged_model_version, job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we (re)load our model since we have just (re)compiled it and (re)packaged it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (re)load the model...\n",
    "print('Reloading Model: ' + model_name + \" using package: \" + model_package + '...')\n",
    "print(\"\")\n",
    "my_notebook.pelion_api.pelion_reload_model(model_name,model_package)\n",
    "\n",
    "# Poll every 5 sec to look for the reload() completion....\n",
    "while True:\n",
    "    print(\"Reloading \" + model_name + \"...\")\n",
    "    is_running = my_notebook.pelion_api.pelion_cmd_is_running(\"reloadModel\");\n",
    "    if is_running == False:\n",
    "        print(\"\")\n",
    "        print('Reload Completed!')\n",
    "        print(\"\")\n",
    "        break\n",
    "    time.sleep(5)\n",
    "\n",
    "# Get the loaded model(s) info...\n",
    "reload_result = my_notebook.pelion_api.pelion_list_models();\n",
    "if 'response' in reload_result and len(reload_result['response']) > 0:\n",
    "    if 'name' in reload_result['response'][0]:\n",
    "        print(\"\")\n",
    "        print(\"Currently Loaded Model(s):\")\n",
    "        print(reload_result)\n",
    "else:\n",
    "    print(\"Nodel: \" + model_name + \" did NOT load properly. Check sagemaker edge agent logs on GW.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do a prediction. We will store the results (with a timestamp) back on S3 so that we can pull it back to our notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the prediction with our input image data\n",
    "input_data = 's3:///' + input_data_filename_saved\n",
    "output_result = 's3:///' + model_basename + '-predicted.data'\n",
    "print(\"Invoking Prediction on Pelion Edge with Sagemaker. Model: \" + model_name + \" Input: \" + input_data + \" Output: \" + output_result)\n",
    "print(\"\")\n",
    "my_notebook.pelion_api.pelion_predict(model_name, input_data, output_result)\n",
    "\n",
    "# Poll every 5 sec to look for the predict() completion....\n",
    "while True:\n",
    "    print('Predicting...')\n",
    "    is_running = my_notebook.pelion_api.pelion_cmd_is_running(\"predict\");\n",
    "    if is_running == False:\n",
    "        print(\"\")\n",
    "        print('Prediction Completed!')\n",
    "        print(\"\")\n",
    "        break\n",
    "    time.sleep(5)\n",
    "    \n",
    "# Now get the prediction result\n",
    "prediction_result = my_notebook.pelion_api.pelion_last_cmd_result();\n",
    "if 'details' in prediction_result:\n",
    "    if 'output' in prediction_result['details']:\n",
    "        print(\"\")\n",
    "        print(\"Prediction Results:\")\n",
    "        print(prediction_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we display our results...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction results tensor filename in our notebook\n",
    "prediction_results_tensor_filename = model_basename + '-prediction-output.tensor'\n",
    "\n",
    "# Copy the results back to our notebook\n",
    "my_notebook.copy_results_to_notebook(prediction_result['details']['output'][0]['url'],prediction_results_tensor_filename)\n",
    "\n",
    "# Read in the output tensor file, convert it, then decode our predictions and display our results...\n",
    "file_size = os.path.getsize(prediction_results_tensor_filename)\n",
    "print(\"Prediction Output File Size: \" + str(file_size) + \" bytes\")\n",
    "with open(prediction_results_tensor_filename, 'r') as file:\n",
    "    # Load the JSON-based tensor from its file in our notebook... \n",
    "    json_tensor = json.loads(file.read())\n",
    "    print(\"\")\n",
    "    print('Output JSON Tensor: Name: ' + json_tensor['name'] + \" Type: \" + str(json_tensor['type']) + \" Shape: \" + str(json_tensor['shape']))\n",
    "    \n",
    "    # Convert our JSON-based output tensor to a numpy float tensor\n",
    "    np_float_tensor = my_notebook.json_tensor_to_numpy_float_tensor(json_tensor)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Decoding Prediction results...\")\n",
    "    most_likely_labels = my_notebook.decode_predictions(np_float_tensor, top=1, class_list_path='./model/imagenet_class_index.json')\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Displaying prediction results...\")\n",
    "    my_notebook.display_images(image_list,most_likely_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!  As a Data Scientist, I could iterate on re-training the model with additional input data, then recompile/deploy/predict to assess training. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.1 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
